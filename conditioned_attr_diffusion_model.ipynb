{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRdfEYDIeCgv"
      },
      "source": [
        "# Making a attribute Conditioned Diffusion Model\n",
        "\n",
        "In this notebook we will implement one way to add conditioning information to a diffusion model. Specifically, we'll train a attribute-conditioned diffusion model on CelebA dataset following on from the [huggingface example](https://github.com/huggingface/diffusion-models-class/blob/unit2/unit1/02_diffusion_models_from_scratch.ipynb), where we make several improvements in terms of model architecture and dataset.\n",
        "\n",
        "This is one of many ways we could add additional conditioning information to a diffusion model, and has been chosen for its relative simplicity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6yLvxXMNf0C",
        "outputId": "a8770fe6-dcc7-48d8-d9f0-181f13c44944"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWqhoaZOeCg3"
      },
      "source": [
        "## Setup and Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GrR0zmfPeCg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f72b686-bad8-448b-9fac-80737de55009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.8 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/265.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q diffusers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR_9YO3TeCg7",
        "outputId": "426e13c4-176a-4748-95f4-62f78f5302d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from diffusers import DDPMScheduler, UNet2DModel\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import os\n",
        "\n",
        "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT86gGlx3eWp"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "img_size = 64\n",
        "transforms = T.Compose([\n",
        "        T.Resize((img_size, img_size)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "# Load the dataset\n",
        "# Using the \"attr\" (attribute) labels here for more control in image generation\n",
        "dataset = torchvision.datasets.CelebA(root=\"celeba/\", split=\"Train\", target_type=\"attr\", download=True, transform=transforms)\n",
        "\n",
        "# Feed it into a dataloader (batch size 8 here just for demo)\n",
        "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View some examples\n",
        "x, y = next(iter(train_dataloader))\n",
        "print('Input shape:', x.shape)\n",
        "print('Sample Label for the first datapoint:', y[0])\n",
        "plt.imshow(torchvision.utils.make_grid(x).numpy().transpose(1,2,0));"
      ],
      "metadata": {
        "id": "DNY9oZP133x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeX8-hv7eCg-"
      },
      "source": [
        "## Creating a Class-Conditioned UNet\n",
        "\n",
        "The way we'll feed in the class conditioning is as follows:\n",
        "- Create a standard `UNet2DModel` with some additional input channels  \n",
        "- Map the class label to a learned vector of shape `(class_emb_size)`via an Linear layer (in place of an embedding layer in popular diffusion models)\n",
        "- Concatenate this information as extra channels for the internal UNet input with `net_input = torch.cat((x, class_cond), 1)`\n",
        "- Feed this `net_input` (which has (`3 + class_emb_size`) channels in total) into the UNet to get the final prediction\n",
        "\n",
        "In this case I've set the class_emb_size to 5, (since tehre is 40 binary attributes => 5 bits encoding information) but this is experimental and different encoding sizes can be explored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpkSrZF5fdmK"
      },
      "outputs": [],
      "source": [
        "class ClassConditionedUnet(nn.Module):\n",
        "  def __init__(self, class_attr=40, class_emb_size=5):\n",
        "    super().__init__()\n",
        "\n",
        "    # The linear layer will map the class attributes to a vector of size class_emb_size\n",
        "    self.class_emb = nn.Linear(class_attr, class_emb_size)\n",
        "\n",
        "    # Self.model is an unconditional UNet with extra input channels to accept the conditioning information (the class embedding)\n",
        "    self.model = UNet2DModel(\n",
        "        sample_size=64,           # the target image resolution\n",
        "        in_channels=3 + class_emb_size, # Additional input channels for class cond.\n",
        "        out_channels=3,           # the number of output channels\n",
        "        layers_per_block=2,       # how many ResNet layers to use per UNet block\n",
        "        block_out_channels=(32, 32, 32, 64),\n",
        "        down_block_types=(\n",
        "            \"DownBlock2D\",        # a regular ResNet downsampling block\n",
        "            \"AttnDownBlock2D\",    # a ResNet downsampling block with spatial self-attention\n",
        "            \"AttnDownBlock2D\",\n",
        "            \"AttnDownBlock2D\",\n",
        "        ),\n",
        "        up_block_types=(\n",
        "            \"AttnUpBlock2D\",\n",
        "            \"AttnUpBlock2D\",      # a ResNet upsampling block with spatial self-attention\n",
        "            \"AttnUpBlock2D\",      # a ResNet upsampling block with spatial self-attention\n",
        "            \"UpBlock2D\",          # a regular ResNet upsampling block\n",
        "          ),\n",
        "    )\n",
        "\n",
        "  # Our forward method now takes the class labels as an additional argument\n",
        "  def forward(self, x, t, class_attr):\n",
        "    # Shape of x:\n",
        "    bs, ch, w, h = x.shape\n",
        "    # print(\"class_attr shape = \", class_attr.shape, class_attr.type)\n",
        "    # class conditioning in right shape to add as additional input channels\n",
        "    class_cond = self.class_emb(class_attr.float().to(device)) # Map to embedding dimension\n",
        "    # print(\"class_cond shape =\", class_cond.shape)\n",
        "\n",
        "    class_cond = class_cond.view(bs, class_cond.shape[1], 1, 1).expand(bs, class_cond.shape[1], w, h)\n",
        "    # x is shape (bs, 3, 64, 64) and class_cond is now (bs, 5, 64, 64)\n",
        "\n",
        "    # Net input is now x and class cond concatenated together along dimension 1\n",
        "    net_input = torch.cat((x, class_cond), 1) # (bs, 8, 64, 64)\n",
        "\n",
        "    # Feed this to the UNet alongside the timestep and return the prediction\n",
        "    return self.model(net_input, t).sample # (bs, 8, 64, 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0YRk5OdeChC"
      },
      "source": [
        "## Training and Sampling\n",
        "\n",
        "We'll now add the attribute labels as a third argument (`prediction = unet(x, t, y)`) during training, and at inference we can pass whatever attributes we want and the model should generate images that match. `y` in this case is the attributes of the CelebA faces, with length 40 and values [-1,1]\n",
        "\n",
        "We predict the noise (rather than the denoised image) to match the objective expected by the default DDPMScheduler which we're using to add noise during training and to generate samples at inference time. Training takes a while - speeding this up could be another project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o323qqFsPE9n"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nHKA027wkrL"
      },
      "outputs": [],
      "source": [
        "# Redefining the dataloader to set the batch size higher than the demo of 8\n",
        "train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "train_dataloader = accelerator.prepare(train_dataloader)\n",
        "# How many runs through the data should we do?\n",
        "n_epochs = 10\n",
        "\n",
        "# Our network\n",
        "net = ClassConditionedUnet().to(device)\n",
        "\n",
        "trainable_params = sum(\n",
        "\tp.numel() for p in net.parameters() if p.requires_grad\n",
        ")\n",
        "\n",
        "print(\"total no.of parameetrs in unet model = \", trainable_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWwX2w0Dwd-C"
      },
      "outputs": [],
      "source": [
        "# Create a scheduler\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training from a Previous checkpoint"
      ],
      "metadata": {
        "id": "U3G_QlmoIBrc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMicr8Fgeywa"
      },
      "outputs": [],
      "source": [
        "# if you want to strat training from a previous checkpoint then run this cell\n",
        "# otherwise skip\n",
        "epoch_no = 10\n",
        "path = \"drive/MyDrive/Colab Notebooks/\"+str(epoch_no)+\"_ckpt.pt\"\n",
        "net.load_state_dict(torch.load(path, map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Core Tarinng Loop\n",
        "\n",
        "For every typle `(x,y)`, we randomly choose a timestep `t` and add the cooresponding noise to the original image `noise_scheduler.add_noise(x, noise, timesteps)`\n",
        "\n",
        "Then the UNet model predicts the amount of noise present in the image. This loss is then backpropagated to update the model. Note that we also pass on the labels/attributes in this case such that it iteratively learns the conditonal information while denoising the image."
      ],
      "metadata": {
        "id": "fUP56QJ2IJYt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcrUOfdtjL3B"
      },
      "outputs": [],
      "source": [
        "# Our loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# The optimizer\n",
        "opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "# Keeping a record of the losses for later viewing\n",
        "losses = []\n",
        "\n",
        "# The training loop\n",
        "for epoch in range(n_epochs):\n",
        "    for x, y in tqdm(train_dataloader):\n",
        "\n",
        "        # Get some data and prepare the corrupted version\n",
        "        x = x.to(device) * 2 - 1 # Data on the GPU (mapped to (-1, 1))\n",
        "        y = y.to(device)\n",
        "        noise = torch.randn_like(x)\n",
        "        timesteps = torch.randint(0, 999, (x.shape[0],)).long().to(device)\n",
        "        noisy_x = noise_scheduler.add_noise(x, noise, timesteps)\n",
        "\n",
        "        # Get the model prediction\n",
        "        pred = net(noisy_x, timesteps, y) # Note that we pass in the labels y\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_fn(pred, noise) # How close is the output to the noise\n",
        "\n",
        "        # Backprop and update the params:\n",
        "        opt.zero_grad()\n",
        "        accelerator.backward(loss)\n",
        "        opt.step()\n",
        "\n",
        "        # Store the loss for later\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # Print out the average of the last 100 loss values to get an idea of progress:\n",
        "    avg_loss = sum(losses[-100:])/100\n",
        "    print(f'Finished epoch {epoch}. Average of the last 100 loss values: {avg_loss:05f}')\n",
        "    torch.save(net.state_dict(), os.path.join(\"./drive/MyDrive/Colab Notebooks/\", str(epoch + epoch_no+1)+f\"_ckpt.pt\"))\n",
        "\n",
        "# View the loss curve\n",
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction\n",
        "\n",
        "This part is pretty straightforward. We start with random noise and iterate over 1000 timesteps to get the real image.\n",
        "\n",
        "We also pass on the attribute labels to impose additional conditonality to the model.\n",
        "\n",
        "Note: In case the outputs are not visible, refer to the final project report in the github repo."
      ],
      "metadata": {
        "id": "AIMfUUJ1JwGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare random x to start from, plus some desired labels y\n",
        "num_img = 24\n",
        "x = torch.randn(num_img, 3, 64, 64).to(device)\n",
        "\n",
        "attr1 = [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
        "         1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1]\n",
        "\n",
        "attr2 = [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1]\n",
        "\n",
        "attr00012 = [0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
        "\n",
        "y = torch.tensor([attr00012]*num_img).to(device)\n",
        "\n",
        "# print(y.shape)\n",
        "# Sampling loop\n",
        "for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
        "\n",
        "    # Get model pred\n",
        "    with torch.no_grad():\n",
        "        residual = net(x, t, y)  # Again, note that we pass in our labels y\n",
        "\n",
        "    # Update sample with step\n",
        "    x = noise_scheduler.step(residual, t, x).prev_sample\n"
      ],
      "metadata": {
        "id": "6kQDPyfe9NKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaxZmuI7d3St"
      },
      "outputs": [],
      "source": [
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = (((x + 1)/2.0)*255).to(torch.uint8)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "ax.imshow(torchvision.utils.make_grid(out.detach().cpu()).permute(1,2,0))"
      ],
      "metadata": {
        "id": "w4PFfE4u6tGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some more visualization"
      ],
      "metadata": {
        "id": "yQcsvjWUKPN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "im = np.array(Image.open(\"./drive/MyDrive/Colab Notebooks/000012.jpg\").resize((64,64)))\n",
        "im = (im/255.0)\n",
        "im000012 = torch.Tensor(np.expand_dims(im, axis=-1)).permute(3, 2, 0, 1).to(device)\n",
        "\n",
        "print(im000012.shape)\n",
        "x = torch.randn(im000012.shape).to(device)\n",
        "\n",
        "x = (im000012 + x)\n",
        "\n",
        "attr000012 = [0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
        "\n",
        "y = torch.tensor([attr000012]).to(device)\n",
        "\n",
        "for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
        "\n",
        "    # Get model pred\n",
        "    with torch.no_grad():\n",
        "        residual = net(x, t, y)  # Again, note that we pass in our labels y\n",
        "\n",
        "    # Update sample with step\n",
        "    x = noise_scheduler.step(residual, t, x).prev_sample\n",
        "\n",
        "\n",
        "out = (((x + 1)/2.0)*255).to(torch.uint8)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
        "ax.imshow(torchvision.utils.make_grid(out.detach().cpu()).permute(1,2,0))"
      ],
      "metadata": {
        "id": "hyQ1NA8q_wIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model on other tasks"
      ],
      "metadata": {
        "id": "u0LE727EKYl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "im = np.array(Image.open(\"./drive/MyDrive/Colab Notebooks/hoi/0368.jpg\").resize((64,64)))\n",
        "im = (im/255.0)\n",
        "im368 = torch.Tensor(np.expand_dims(im, axis=-1)).permute(3, 2, 0, 1).to(device)\n",
        "\n",
        "print(im368.shape)\n",
        "x = torch.randn(im368.shape).to(device)\n",
        "\n",
        "x = im368\n",
        "\n",
        "attr000012 = [0]*40\n",
        "\n",
        "y = torch.tensor([attr000012]).to(device)\n",
        "\n",
        "image_viewer = []\n",
        "\n",
        "for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
        "\n",
        "    # Get model pred\n",
        "    with torch.no_grad():\n",
        "        residual = net(x, t, y)  # Again, note that we pass in our labels y\n",
        "\n",
        "    # Update sample with step\n",
        "    x = noise_scheduler.step(residual, t, x).prev_sample\n",
        "    image_viewer.append(x)\n",
        "\n",
        "out = (((x + 1)/2.0)*255).to(torch.uint8)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
        "ax.imshow(torchvision.utils.make_grid(out.detach().cpu()).permute(1,2,0))"
      ],
      "metadata": {
        "id": "MKZj-fLZ4sEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = []\n",
        "for i in range(len(image_viewer)):\n",
        "  if i%50 == 0:\n",
        "    x = image_viewer[i]\n",
        "    out.append((((x + 1)/2.0)*255).to(torch.uint8))\n",
        "\n"
      ],
      "metadata": {
        "id": "NQGApHlogtud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# out = torch.tensor(np.asarray(out, dtype=np.float32))\n",
        "# print(np.asarray(out).shape)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
        "ax.imshow(torchvision.utils.make_grid(out[5].detach().cpu()).permute(1,2,0))"
      ],
      "metadata": {
        "id": "FvIy1zNGie1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, numpy, PIL\n",
        "from PIL import Image\n",
        "\n",
        "# Access all PNG files in directory\n",
        "allfiles=os.listdir(\"./drive/MyDrive/Colab Notebooks/hoi\")\n",
        "imlist=[filename for filename in allfiles if  filename[-4:] in [\".jpg\"]]\n",
        "\n",
        "# Assuming all images are the same size, get dimensions of first image\n",
        "w,h=Image.open(os.path.join(\"./drive/MyDrive/Colab Notebooks/hoi\",imlist[0])).resize((256,256)).size\n",
        "N=len(imlist)\n",
        "\n",
        "# Create a numpy array of floats to store the average (assume RGB images)\n",
        "arr=numpy.zeros((h,w,3),numpy.float)\n",
        "\n",
        "# Build up average pixel intensities, casting each image as an array of floats\n",
        "for im in imlist:\n",
        "    imarr=numpy.array(Image.open(os.path.join(\"./drive/MyDrive/Colab Notebooks/hoi\",im)).resize((256,256)),dtype=numpy.float)\n",
        "    arr=arr+imarr/N\n",
        "\n",
        "# Round values in array and cast as 8-bit integer\n",
        "arr=numpy.array(numpy.round(arr),dtype=numpy.uint8)\n",
        "\n",
        "# Generate, save and preview final image\n",
        "out=Image.fromarray(arr,mode=\"RGB\")\n",
        "out.save(\"Average.png\")\n",
        "out.show()"
      ],
      "metadata": {
        "id": "7GexHMKbjUfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imarr=Image.open(os.path.join(\"./drive/MyDrive/Colab Notebooks/hoi/0038.jpg\")).resize((256,256))\n",
        "\n",
        "\n",
        "imarr.save(\"oneimage.png\")\n",
        "imarr.show()"
      ],
      "metadata": {
        "id": "U49H_xk8kxMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A4Nfl-a5odMb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}